#+TITLE: core dump

* Understanding a Trait Selection Bug
:PROPERTIES:
:RSS_PERMALINK: trait_selection_bug.html
:PUBDATE:  2024-04-21
:ID:       89c732d4-6f54-42fb-b0e0-d9568bab7664
:END:
#+setupfile: ../basic-setup.org
Recently, I ran into [[https://github.com/rust-lang/rust/issues/24066][rust-lang/issues/#24066]] and spent some time analysing the underlying trait selection mechanism. In this blog post, I'll briefly explain why this bug prevents the following snippet from compiling.

#+begin_src rust
  fn add<T>() where
      u32: std::ops::Add<T, Output = T>
  {
      let _ = 1_u32 + 1_u32; // error[E0308]: mismatched types
  }
#+end_src

Inside the compiler, the ~rustc_hir_typeck~ crate handles lookup of methods for binary operations. When it encounters the expression ~1_u32 + 1_u32~, it does something peculiar: instead of directly looking up for a method that adds an ~u32~ to another ~u32~, it searches for a method that can add an ~u32~ to a type inference variable ~$0~. This allows the compiler to either borrow or take ownership of the value on the RHS of the binary expression based on the available methods[fn:: https://github.com/rust-lang/rust/blob/0824b300eb0dae5d9ed59719d3f2732016683d66/compiler/rustc_hir_typeck/src/op.rs#L238-L254].

The request "find a method to add an ~u32~ with the type inference variable ~$0~" is handled by the ~rustc_trait_selection~ crate, which selects an appropriate method from a collection of candidates. Here, traits specified in the where clause are given a higher precedence over other candidates[fn:: https://doc.rust-lang.org/stable/nightly-rustc/rustc_middle/traits/select/enum.SelectionCandidate.html]. I believe this heuristic is followed as the programmer is expected to specify only the necessary traits in the where clause.

Thus, for the expression ~1_u32 + 1_u32~, a method which adds an ~u32~ and a type variable ~T~ is chosen because it's mentioned in the where clause and ~T~ unifies with the type inference variable ~$0~. This attempts to unify the ~u32~ on the RHS with type variable ~T~, resulting in a type mismatch error.

Guess, this is an other instance where a compiler leans towards soundness over completeness.
* Understanding Mesh Allocator
:PROPERTIES:
:RSS_PERMALINK: mesh_allocator.html
:PUBDATE:  2024-01-24
:ID:       59afcb5d-b833-44a2-904e-446327cd66ed
:END:
#+setupfile: ../basic-setup.org
I recently read about the [[https://github.com/plasma-umass/Mesh][Mesh memory allocator]] and was fascinated by its ability to perform compaction without changing the addresses of allocated objects.

In this post, I will briefly summarize how Mesh allocator works. For more details about its algorithms and implementation, refer to the [[https://raw.githubusercontent.com/plasma-umass/Mesh/master/mesh-pldi19-powers.pdf][paper (PDF)]] by Bobby Powers et al. or watch this [[https://www.youtube.com/watch?v=xb0mVfnvkp0][talk]].
Fragmentation
Applications written in manually memory managed languages might fail to allocate memory, even when the required amount of space is available in memory. This problem is known as memory [[https://en.wikipedia.org/wiki/Fragmentation_(computing)][fragmentation]] and can be solved by compaction, where all allocated objects are moved together to make a contiguous block of free space available.

Garbage collected languages can solve this by stopping program execution to compact allocated objects and update all pointers to point to the relocated objects. But, languages which expose raw pointers to the programmer don't have this luxury as it is impossible to trace and update all the pointers.

Therefore, to reduce memory fragmentation for applications written in manually memory managed languages, compaction should occur without changing the pointers' values and this is exactly where Mesh shines.
Compaction Without Relocation
The Mesh allocator performs compaction without relocation by taking advantage of the fact that pointers are virtual memory addresses pointing to objects on physical memory. This allows the allocator to move objects in the physical memory and update the virtual to physical memory mapping without changing the virtual memory address of allocated objects.

There are three core concepts behind the Mesh allocator:
** Meshing
:PROPERTIES:
:ID:       33cee529-ea6e-44f9-a3fd-269d890a299f
:END:
Meshing is the phase where objects are relocated in physical memory. It randomly selects physical memory pages with objects allocated at non-overlapping offsets and merges (meshes) them together by copying all objects from one page to another. The copied objects retain their original offsets. Then, the page from which content was moved is given back to the operating system; effectively reducing memory consumption.

This does not affect performance as meshing is performed concurrently without stopping program execution.
** Updating Virtual to Physical Address Mapping
:PROPERTIES:
:ID:       4db13997-946f-4be7-8ece-43f0d23b9c4e
:END:
After meshing, the process's page table is updated to remap the virtual memory addresses of moved objects to the new physical addresses. However, the program is unaware of this change because the virtual memory addresses (pointers) remain the same since the offsets of objects have not changed.
** Randomized Allocation
:PROPERTIES:
:ID:       1d65b2b8-46fa-4ccd-a120-a67cb99589f7
:END:
Memory allocation is randomly spread throughout a page to facilitate meshing by increasing the chances of pages having objects allocated at non-overlapping offsets. This doesn't [[https://news.ycombinator.com/item?id=19182779#19185084][seem]] to affect cache performance as allocations returned from popular memory allocators are not contiguous either.
Conclusion
This memory allocator has proven to significantly reduce the memory consumption of Firefox and Redis with only a small increase in performance overhead.

I'm amazed at how simple and easy-to-understand techniques can significantly reduce fragmentation while maintaining performance, without requiring any changes to the codebase.
* Boba Devlog #2: Compiling Arrays
:PROPERTIES:
:RSS_PERMALINK: array.html
:PUBDATE:  2023-09-04
:ID:       9dd96c50-a830-49ca-aa3b-9e89a12c0a51
:END:
#+setupfile: ../basic-setup.org
[[https://github.com/veera-sivarajan/boba][Boba]] is a compiler project I'm working on to learn how to implement type system, code optimization passes and code generation. It compiles a language with Rust-like syntax and C-like semantics to x86-64 assembly. In this post, I'd like to explain how it compiles arrays.
Syntax and Semantics of Arrays
The syntax for arrays resembles that of Rust and the semantics follow that of C.

An array is a collection of elements stored in a contiguous block of memory on the stack. It evaluates to a pointer to the first element in the array and this pointer can be assigned to other variables, passed into functions and used in subscript expressions to access any element of the array. But, returning an array (pointer) from a function will result in garbage values or segmentation fault. When an array is passed into a print statement, it will print all the elements in the array.
#+begin_src rust
  fn sum(array: [i32; 5]) -> i32 {
      let mut total = 0;
      for (let mut i = 0; i < 5; i = i + 1) {
          total = total + array[i];
      }
      return total;
  }

  fn main() {
      let array = [1, 2, 3, 4, 5];
      let total = sum(array);
      println("Sum of {} = {}", array, total); // Sum of [1, 2, 3, 4, 5] = 15
  }
#+end_src
Memory Layout
To understand how arrays can be expressed in assembly, it helped me a lot to visualize the memory layout of arrays.
** One-dimensional Array
:PROPERTIES:
:ID:       0e7ea682-2e08-47a7-b07e-de965b93b1dd
:END:
The code ~let a = [1, 2, 3, 4];~ will have the following memory representation:
[[file:./imgs/one-d-array.png]]
Here, ~rbp~ is the base pointer of the current stack frame and ~-4[rbp]~ can be interpreted as four bytes below the base pointer.

Each number in the array is stored in a four byte interval between ~rbp~ and ~-16[rbp]~ and the variable ~a~ (pointer to the first element in the array) takes up the eight bytes immediately after ~-16[rbp]~. The address of each value decreases since the stack grows downward.
** Two-dimensional Array
:PROPERTIES:
:ID:       7e74165b-f6e8-45ab-bbbe-3547dd6becfe
:END:
The representation of a two-dimensional array is similar to an one-dimensional array but there is a level of indirection to take care of. For example, the code ~let a = [[1, 2], [3, 4]];~ will be represented as:
[[file:./imgs/two-d-array.png]]
The first element in the array is a pointer to the first array and the second element is a pointer to the second array. The data is tightly packed because the addresses naturally align for the numbers (four bytes each) and pointers (eight bytes each). When the values don't align naturally there will be some padding added in between the values.
Subscript Expressions
A subscript expression ~numbers[i]~ is used to access the element at index ~i~ in array ~numbers~. On the assembly level, this is expressed by dereferencing the pointer obtained by adding the pointer to the first element in ~numbers~ with the product of ~i~ and the size of an element in ~numbers~.

Internally, this gets compiled to the ~lea~ instruction to obtain the offset by multiplying the index and element size, the ~add~ instruction to increment the array pointer with the offset and the indirect addressing mode to dereference the pointer.
Printing Arrays
An array is just a pointer but when it is passed into a print statement, the programmer, for the most part, expects all the elements in the array to be printed rather than the pointer address. Also, since the length of the array and the type of each element in the array is known at compile time, the compiler can easily access all the elements of the array given its base pointer.

For example, if an array ~let array = [1, 2, 3, 4];~ is passed into print as:
#+begin_src rust
  println("{}", array);
#+end_src
The print statement can be transformed into:
#+begin_src rust
  println("[%d, %d, %d, %d]", array[0], array[1], array[2], array[3]);
#+end_src

A small obstruction to express the transformation in assembly is the System V ABI calling convention where the first six arguments to a function are passed in registers and the rest are pushed onto the stack in reverse order. But, for an array where the first few elements are passed in registers and the rest are pushed onto the stack, it is complicated for the compiler to evaluate the first few elements in left-to-right order and then evaluate the rest that goes onto the stack in right-to-left order[fn::Maybe there is an efficient and elegant way to implement this but I cannot figure it out.]. 

So, I decided to evaluate all arguments to a print statement (including an array's elements) in right-to-left order. This allowed me to iterate over the arguments only once and place all the arguments in appropriate registers and positions on the stack.
* Boba Devlog #1: Sliding In a Type Checker
:PROPERTIES:
:RSS_PERMALINK: type_checking.html
:PUBDATE:  2023-07-31
:ID:       a78acbef-6b6c-4fb2-ac4d-64c9d3278c14
:END:
#+setupfile: ../basic-setup.org
[[https://github.com/veera-sivarajan/boba/tree/main][Boba]] is a compiler project I'm working on to learn how to implement type system, code optimization passes and code generation. It compiles a language with Rust-like syntax and C-like semantics to x86-64 assembly. Check out the [[https://github.com/veera-sivarajan/boba/blob/main/language.org][language documentation]] to know more about syntax and semantics of the language.

This month I implemented a type checker and it helped me to make a number of improvements to the language. Now, the compiler can handle more primitive data types and also a Rust style format string for print statements. In this post, I will explain everything I learnt over the month.
Learning x86-64 Assembly
Before starting this project, I did not know anything about assembly. So, to learn how various language constructs can be expressed in assembly, I'd write a simple C program and compile it using ~gcc -O0 -fverbose-asm -S~. The assembly file produced by this command shows each line of the source code (in comments) and its equivalent unoptimized assembly right below it. This helped me a lot to understand how function call and stack alignment work at the assembly level.
Type Checking
Initially, I thought I should implement Hindley-Milner type inference algorithm to infer the types of all expressions because the language is implicitly typed. But, after reading a [[https://lobste.rs/s/ewxw1i/less_technical_introductions_type#c_hotmwn][comment]] by matklad, I realized I don't need that since there are no generics in my language. 

Therefore, my current implementation walks through every node in the AST like a tree-walk interpreter but instead of evaluating primary expressions to their values, it evaluates them to their types. For example, the expression ~Expr::Number(23)~ evaluates to ~Type::Number~. The types are then bubbled-up the AST and checked if they are equal to the expected type. If a type does not match the expected type, an appropriate error is added to a list and type checker continues to check the rest of the program. After the whole program is analyzed, the compiler prints out all the errors in the list. If there is no error, the AST along with all the type information is passed on to the code generator to generate the assembly. The code generator will use the type information of each expression to choose the correct instruction suffix and register size. As an example, the code generator will choose ~movl~ and ~%eax~ for returning a 32 bit number type instead of ~movq~ and ~%rax~.
Fancy Print Statement 
Additionally, implementing a type checker paved way for Rust style fancy format strings in print statements even though the compiler calls ~printf()~ to display values to ~stdout~.
#+begin_src rust
  println("{}. Your name is: {}", some_number, some_string);
#+end_src
Since the compiler can infer the types of both ~some_number~ and ~some_string~, the format string is replaced with appropriate C format specifiers and compiled to ~"%d. Your name is: %s\n"~ before being passed as first argument to ~printf()~. Finally, the values to be printed get passed as subsequent arguments to ~printf()~ without any changes because the primitive types in my language are compatible with C ABI.
Fixing Stack Alignment
I first encountered this bug when calling ~printf()~. The program would abruptly crash when a function has a certain number of local variables and it was very confusing because there wasn't anything wrong in the generated assembly in terms of space allocated on the stack or using the correct instructions.

Turns out the System V ABI requires the stack to be aligned to a 16 byte boundary inside every function. So my mistake was allocating the exact amount of space required for local variables as a function with three boolean values will allocate three bytes plus forty eight bytes for callee saved registers and leave the stack misaligned. To fix this, the code generator rounds the sum of space required for local variables and callee saved registers to the nearest multiple of 16 before allocating. In the function epilogue, the rounded up amount of space gets deallocated from the stack.

This solved the issue and now the programs don't crash for any number of local variables.
* Boba Devlog #0: Compiling Pseudo-Rust to x86 Assembly
:PROPERTIES:
:RSS_PERMALINK: codegen.html
:PUBDATE:  2023-06-30
:ID:       57b1a5da-9203-4edb-9e15-bcd26c8f5d92
:END:
#+setupfile: ../basic-setup.org
[[https://github.com/veera-sivarajan/boba][Boba]] is a compiler I'm writing to learn how high-level language constructs can be expressed in assembly. It compiles Rust-like source code to 64-bit x86 instruction set. This post will describe the current status of the project.
Language Overview
The language's syntax looks like Rust, but semantically its a lot like C with one big difference: here, the compiler does not care about declaration order in the global scope, so functions can be called before they are defined in the source code.
#+begin_src rust
  // Example program that can be compiled with Boba
  fn main() -> i64 {
      println(factorial(5));
      return 0;
  }

  fn factorial(num: i64) -> i64 {
      if num <= 1 {
          return 1;
      } else {
          return num * factorial(num - 1);
      }
  }
#+end_src
Compiler Architecture
The compiler follows a traditional multi-pass architecture:
Scanner walks through every character in the source code and builds a list of tokens.
Parser builds a high-level AST by parsing the tokens with a recursive descent LL(1) parser. This step is very similar to parsing Lox in Crafting Interpreters.
Analyzer performs a bunch of static checks to verify that all the variables and functions in use are defined. Also, local variable names are resolved to their offset from the base register pointer ~%rbp~. This step returns a low-level AST iff there are no semantic errors in the source code.
Code generator iterates over every node in the low-level AST and translates it to assembly.

The high-level AST has token information which can be used to point an error message to a particular line and column number in the source code but the low-level AST gets rid of all token information and only knows about labels and offset from base pointer. The generated assembly is linked with the C standard library and converted to an ELF executable with the help of ~gcc~. This allowed me to implement ~println()~ as a built-in function that calls ~printf()~. Also, in theory, the generated code can call any C function because it follows the System V calling convention but I have not tested it yet.
Future Work
There aren't any optimization passes, so the generated code is verbose and inefficient. For example, a simple local variable declaration ~let x = 10~ gets translated into:
#+begin_src asm
  movq  $10, %rbx
  movq  %rbx, -8(%rbp)
#+end_src
This is because the code generator generates code for every node in the low-level AST. It has no way to know that those two instructions are related and can be combined into one. Also, there is no type system as 64-bit signed number is the only data type supported by the compiler.
Reference
Chapters 10 - 12 in [[https://www3.nd.edu/~dthain/compilerbook/][Introduction to Compilers and Language Design]] by Douglas Thain gave me a good idea about assembly language, code generation and optimization.
* Idempotency in the Wild
:PROPERTIES:
:RSS_PERMALINK: idempotence.html
:PUBDATE:  2023-05-07
:ID:       cd7260ba-5a97-4817-a117-7c8619642258
:END:
#+setupfile: ../basic-setup.org
I learned about /idempotence/ when reading some blog post about Lisp. It is used to denote operations which have the same effect for any number of times they are executed. For example, emptying a glass of water.

I came across a neat real-life example of an idempotent system on my bus ride to home after classes. There is a chord that runs through the length of the bus which the riders can pull to request the driver to stop at the next stop. When a rider pulls the chord, it immediately makes a loud announcement on the speakers to notify all the riders that the bus is about to stop. Until the bus arrives at the next stop, subsequent chord pulls will not make any announcement but instead flash a [[stop requested sign][stop requested sign]] on the LED display to signal that the driver is aware of the stop request. Thereby, any number of stop requests will make only one announcement.

#+caption: stop requested sign
#+name: stop requested sign
[[file:./imgs/stop-requested.jpg]]
* Professor Eliot Moss on the Design and Implementation of CLU
:PROPERTIES:
:RSS_PERMALINK: clu.html
:PUBDATE:  2023-04-06
:ID:       90147c1c-f854-46b5-923b-3b31b5ebeb0f
:END:
#+setupfile: ../basic-setup.org
In the 1970s, Barbara Liskov, along with her graduate students, Eliot Moss and others created the CLU programming language. Its ideas has been adopted in every programming language we use today.

Luckily, I got to take a compilers class with Prof. Eliot Moss last Spring and ask him about his experience working on the design and implementation of the language. The following was his response[fn::Posting here with his permission.]:

#+begin_verse
Hello, Veera!

> I came across the CLU programming language when researching about how
> iterators came into being and was excited to see that you have contributed
> to the design and implementation of CLU.

Yes, though I came on somewhat in the middle of the language design effort.

> Can you please tell me about your experience working on the language? And
> how do you feel about it now that CLU's ideas are adopted in almost every
> modern programming language?

Prior to working on Clu I had coded in Basic, Fortran, Lisp, and C.  The key
innovation of Clu was its emphasis on *abstract data types*, which wrapped
together (into a "cluster", hence the name Clu) a data structure definition
and operations on that data structure into a higher level thing.
Object-orientation is similar in that you can have private parts of your data
structure and then write methods.  What OO languages add to what Clu did is
inheritance / extension of one type by another.  (Clu did have a nice system
of *parameterized* types, similar to Java's type parameters.)

Another nice thing Clu had was a way to extend the builtin operators, like +,
to any data type - we called that "syntactic sugar".  It's a bit like what you
can do in C++, minus the complicated overloading.

Coming back to what you led with: iterators.  We realized that people do like
to iterate over organized collections of things, and also that if we did *not*
provide something like iterators then data abstraction would not be so nice
since people would tend to expose internal data structure to support the
iteration.  Clu's iterators are as easy to *use* as those of Java, but much
easier to *write* because of the yield statement.  In Java, you have to come
up with a way of representing the state of the iteration explicitly, while in
Clu it is implicit in the local variables, etc., of the running iterator.  It
did require a more co-routine / lambda function kind of implementation (the
caller of an iterator has to provide a place for the iterator to "call" back
into the loop body).

Clu was also a garbage collected language - I wrote the first garbage
collector for it (in assembly language, no less!).  Storage management errors,
and just the general mess of tracking your storage, makes it harder to write
programs.  In Clu, many of us had the novel experience of having a program
work the first time it compiled (!).  Clu's nice collection of builtin types
and the functions provided for them also made many things easier to code, an
example being strings.  (This was part where I took the design lead, as I
recall.)

Overall, it was quite an elegant language, if I say so myself.  And for
someone who works in the PL area, having a grad school experience of being on
a good language design and implementation team was great!  I am grateful to
all of them, and of course our advisor and group leader, Barbara Liskov.

Cheers - E
#+end_verse
* Tracking Music History
:PROPERTIES:
:RSS_PERMALINK: tracking_music_history.html
:PUBDATE:  2023-03-25
:ID:       6d8428f9-f36f-4b33-9369-6fbddaa39f68
:END:
#+setupfile: ../basic-setup.org
I have all my music downloaded on the computer and listen to them using a lightweight music player. This setup worked fine for a while but soon I felt a need for a way to track my music listening history. I wanted a simple tool to log all the music I listen to and view it through an interface that allows me to query using different parameters. 

So I wrote a small daemon that watches for music files I open using ~inotify~ API and logs the song title along with current date and time to a local SQLite database. To view my listening history, I wrote a web interface that can query the database through a bunch GET routes. For example, accessing the ~/top/5~ route will display my top five most listened tracks in a HTML table decorated with minimal CSS.

The daemon was built using the [[https://docs.rs/daemonize/latest/daemonize/][daemonize]] library as it gives nice declarative APIs to configure and spin up a daemon quickly. For the web interface, I used [[https://rocket.rs/][Rocket.rs]] as using attributes to match against paths and the built-in mechanism to pass database connection between various request handlers made the application simple and easy to understand.

To conclude, despite it being a [[https://github.com/veera-sivarajan/music-daemon][small project]], I learnt a lot and had fun working on it.
* Type Inference
:PROPERTIES:
:RSS_PERMALINK: type_inference.html
:PUBDATE:  2023-01-31
:ID:       88740110-7bb7-42f7-86fd-ac8cbf8362f2
:END:
#+setupfile: ../basic-setup.org
Dabbling with Haskell inspired me to learn about type inference. So this last week I spent some time reading through literature on type systems and [[https://github.com/veera-sivarajan/type-inferencer][implemented]] the Hindley-Milner algorithm for a toy expression language using Rust. In this post, I will give an implementation focused explanation of the algorithm. 
Hindley-Milner
The type inference algorithm takes the AST as input and outputs the type for each expression in the tree. Much like a detective, who solves a case by collecting all the available evidence and then finding the relation between them to come to a conclusion, the algorithm creates a mapping between each expression and its expected type and then finds the types for all expressions using the relation between the generated mappings. Formally, the first step is termed as /constraint generation/ and the latter is /unification/.
** Constraint Generation
:PROPERTIES:
:ID:       a6451c89-8331-40fe-9c56-9c98775dbcaa
:END:
A constraint is a mapping between an expression and its expected type. To generate constraints for an AST, the algorithm recursively visits every expression in the tree and maps it to an expected type. Primitive expressions like numbers, booleans and identifiers, can be straightforwardly mapped to their respective types since any symbol parsed as, for example, a number is guaranteed to be of type number. It gets more interesting when generating constraints for nested expressions. 

Constraints for nested expressions are generated in two steps:
Each sub-expression is constrained to an expected type based on its value. For example, the sub-expression ~1~ in a conditional expression will be mapped to type ~Number~.
All sub-expressions and the root expression are mapped to their expected types based on their position in the AST.  For example, if there is a sub-expression ~1~ in the condition part of conditional expression, it will be mapped to type ~Bool~.
*** Binary Add Expression
:PROPERTIES:
:ID:       79bbaa85-be6d-44ae-b28e-033b4e944c4f
:END:
For the binary add expression, generate constraints for the left and right operands and then map each operand and the add expression to type number. For example, the following are the constraints generated for the incorrect expression ~1 + false~.
#+begin_src shell
  1         = Number
  false     = Bool
  1         = Number
  false     = Number
  1 + false = Number
#+end_src
*** Conditional Expression
:PROPERTIES:
:ID:       4267ed8e-9026-4c8a-ada8-69a908d20b24
:END:
For conditional expressions, there are three expectations for its types:
Condition should be of type boolean.
<<2>>Both the branches should be of same type[fn:: Statically typed languages have this requirement because the algorithm cannot determine which branch will get executed during runtime but it has to assign a type for the entire expression. In contrast, dynamically typed languages are more flexible as they can assign the type of conditional expression to the type of the branch that will get executed.].
<<3>>Type of conditional expression should be of same type as its branches.

To express these expectations in terms of constraints, start by generating constraints for the condition and the two branches. Then, map the condition to type boolean and the conditional expression to then and else branch to denote expectations [[2]] and [[3]] from above. The constraints generated for the expression ~if true {1} else {2}~ are:
#+begin_src shell
  true                 = Bool
  1                    = Number
  2                    = Number
  true                 = Bool
  if true {1} else {2} = 1
  if true {1} else {2} = 2
#+end_src
*** Functions
:PROPERTIES:
:ID:       3398c790-60d4-4c07-964e-7abbbcb1e060
:END:
A function definition is constrained as an arrow type with parameters as domain and body as range. As you may be familiar by now, start by generating constraints for parameters and body to determine their types. Then, map the function definition to an arrow type with parameters as domain and body as range. For the expression ~lambda(x) x + 2~, the constraints are:
#+begin_src shell
  x                 = x
  2                 = Number
  x                 = Number
  2                 = Number
  x + 2             = Number
  (lambda(x) x + 2) = x -> x + 2
#+end_src

Similarly, a function call is also constrained as an arrow type with arguments as domain and the call expression as range because the expected type of a call expression is the type returned by calling the function with given arguments. So, generate constraints for the calling function, arguments and then map the function expression to an arrow type with arguments as domain and the call expression as range. For example, calling the function defined above with argument ~10~ will generate:
#+begin_src shell
  x                 = x
  2                 = Number
  x                 = Number
  2                 = Number
  x + 2             = Number
  (lambda(x) x + 2) = x -> x + 2
  10                = Number
  (lambda(x) x + 2) = 10 -> Call<(lambda(x) x + 2)(10)>
#+end_src
** Unification
:PROPERTIES:
:ID:       a22e7acf-7cf4-486b-a8e3-5c102dd52cdb
:END:
The meat of type inference happens during unification. In this step, the algorithm iterates through the list of constraints and outputs either a type error, if any, or the type for each expression.
*** Substitution
:PROPERTIES:
:ID:       c234b0b1-0612-4a3d-8443-60467fea1aa5
:END:
The core idea of unification is a simple concept called substitution. Assume we have the following set of constraints for a function definition ~(lambda(x) x + 2)~.
#+begin_src shell
  1. x                 = Number
  2. 2                 = Number
  3. x + 2             = Number
  4. (lambda(x) x + 2) = x -> x + 2
#+end_src
After unification, the RHS of constraint #4 will be substituted with types asserted by previous constraints. The parameter ~x~ will be substituted with type ~Number~ as stated by constraint #1 and the body ~x + 2~ will also be substituted with type ~Number~ as stated by constraint #3.
#+begin_src shell
  (lambda(x) x + 2) :: Number -> Number
#+end_src
*** Occurs Check
:PROPERTIES:
:ID:       7da8aeef-dc3b-4c87-8902-5e159475a18c
:END:
We substitute a term A with term B to ensure that expressions progressively get mapped to a more specific type. But if term B contains term A, the replacement is redundant and can lead to infinite unifications. To prevent this, ensure that term B does not contain term A anywhere in its nested structure before each substitution.
*** Algorithm
:PROPERTIES:
:ID:       3871d89e-dbb2-46ce-b743-33375a8295f2
:END:
The goal of unification is to make the required substitution for each constraint. The algorithm to accomplish this can be better explained through psuedocode:
#+begin_src python
  substitution list = []
  for LHS and RHS in each constraint:
      if LHS == RHS:
          continue
      else if LHS is not a type:
          substitute all occurrences of LHS with RHS in both constraint and substitution list
          add the substitution LHS -> RHS to substitution list
      else if RHS is not a type:
          substitute all occurrences of RHS with LHS in both constraint and substitution list
          add the substitution RHS -> LHS to substitution list
      else if both LHS and RHS are arrow types:
          create a new constraint mapping domain of LHS with domain of RHS
          create a new constraint mapping range of LHS with range of RHS
          add both constraints to constraint list 
      else:
          it's a type error as LHS and RHS cannot be unified
#+end_src
*Notes*
When replacing all occurrences of LHS with RHS or vice versa, ensure that replacement occurs in both LHS and RHS of constraint and substitution list.
The implementation can take advantage of nested structure of ASTs and implement unification and all supporting functions recursively.
Conclusion
It took me about a week to understand and implement this algorithm. In the end, it felt magical to programmatically infer all the types for an untyped language. 

Refer to the following resources to learn more about this algorithm:
Chapter 30 in [[https://cs.brown.edu/~sk/Publications/Books/ProgLangs/2007-04-26/plai-2007-04-26.pdf][Programming Languages: Application and Interpretation (first edition)]] for step by step examples of constraint generation and unification.
Chapter 15.3.2 in [[https://cs.brown.edu/courses/cs173/2012/book/types.html#%28part._.Type_.Inference%29][Programming Languages: Application and Interpretation (second edition)]] for an explanation of the algorithm using Typed Racket. 
[[https://eli.thegreenplace.net/2018/unification/][Unification]] by Eli Bendersky talks a bit about efficiency.
[[https://github.com/vkz/PLAI/blob/master/type-unify.rkt][type-unify.rkt]] for a complete implementation in Typed Racket. 
* A Programmer's Utopia
:PROPERTIES:
:RSS_PERMALINK: a_programmer's_utopia.html
:PUBDATE:  2022-08-15
:ID:       59f17ee6-4386-410f-8a68-6192f8a333d0
:END:
#+setupfile: ../basic-setup.org
Amidst all the chaos in the world there is a community of passionate programmers who are constantly working on interesting projects, learning new topics and have a great sense of camaraderie. People in this community, driven by their self motivation to become better programmers, do a lot of pair programming, meet with people who share interests to brainstorm ideas and give talks about their projects. This beautiful community is called as [[https://www.recurse.com/][Recurse Center]]. 

Starting in May, 2022, I had the privilege of spending twelve weeks with the community (virtually) to learn and work on projects that caught my fancy. In this post, I will give an overview of my experience. 
Applying to RC
Initially, when I read through [[https://www.recurse.com/about][RC's philosophy]], I was delighted to know that such a place exists because I also had similar ideas about education. As a college student, I've always felt like attending classes, doing homeworks and the assigned projects, and writing exams were not adequate enough for me to learn a concept. I can be confident in my understanding of a concept only if I apply it in a project I'm working on as I was able to sincerely appreciate concepts like functional programming and high order functions only after writing a LISP interpreter. 

Due to my strong alignment with RC's philosophy, I believed I had a good chance to get admitted into RC and I indeed did. I was on cloud nine when I got admitted because it gave me a sense of validation for who I was as a person. 
First six weeks
In the first six weeks of my batch, I spent most of the time interacting with the community and reading [[https://www.cs.rochester.edu/~scott/pragmatics/][Programming Language Pragmatics]]. I also contributed to [[https://github.com/SerenityOS/jakt][Jakt]] programming language and was happy to have a few commits merged into a popular open source project. Finally, I gave a couple of talks about my interpreter project and [[https://youtu.be/ZgEDMpxWvN4][scanner theory]] to improve my presentation skills. 
** Compilers and Interpreters Implementation
:PROPERTIES:
:ID:       470c0da1-ec61-4cb4-932c-0f480f4a7340
:END:
In this weekly event, I met other Recursers interested in writing compilers and we discussed about Haskell, OCaml, Rust, logic languages, parser combinators, and interesting research papers in the field. This event introduced me to a number of other resources I could use to build upon what I learnt so far from reading Crafting Interpreters. By the end, I was grateful to have met people who share my interests. 
** Music Consumption Group 
:PROPERTIES:
:ID:       a5a70663-579a-40e5-9beb-d0641ab8d707
:END:
A weekly social event where a bunch of Recursers hop on a Zoom call, queue a track of their choice and the host will play each track in order. The event was late in the evening so it was fun to listen to a variety of music while cooking dinner or fixing a gnarly bug. 
Last six weeks
I spent most of the time working on [[https://github.com/veera-sivarajan/bessy][Bessy]], a bytecode interpreter written in Rust. I frequently sought help from other Recursers and their guidance helped me to write tests for I/O functions and to add a [[https://veera.app/bessy/][web interface]] for my interpreter by compiling it to Wasm.
** Rust Rodeo
:PROPERTIES:
:ID:       7fc0ca39-28d4-4de4-a689-80a2d8da4d98
:END:
A weekly event to discuss everything related to Rust. Here we helped each other get unstuck on compiler errors and read through popular Rust code bases to improve our knowledge about the language and its ecosystem. Occasionally, someone gave a talk on something cool they learnt about Rust. Through the course of this event, I found myself learning more about generic types and trait system which in turn helped me to write more idiomatic Rust for Bessy.
** Zig Zones
:PROPERTIES:
:ID:       d93aff17-bfcc-4bdd-b7b8-5119aaad6ad9
:END:
This was an interest group for Zig programming. Initially, I was curious to learn what Zig has to offer in terms of an improvement over C and was fascinated by its C interoperability as a first class feature. I played with the toolchain and, with the help of Recursers, I was able to compile a existing C code base using ~zig cc~ and also transpile it to Zig. You can read more about our process to translate a C project into Zig in Jeff Fowler's [[https://blog.jfo.click/zig-translate-c-and-linking-equal-best-friends/]["Zig translate-c and linking equal best friends"]].
** Scone Production and Consumption Group
:PROPERTIES:
:ID:       66807919-eaa7-44a7-b0f4-cc98435e7622
:END:
This was an occasional social event where I baked Scones along with other Recursers by following the [[https://www.seriouseats.com/bakery-style-cream-scones-with-chocolate-recipe][recipe]] written by Stella Parks. This was my first attempt at baking and I really enjoyed the process even though my Scones ended up looking like giant cookies because I did not follow the recipe word for word :). Nevertheless, I see strong parallels between programming and cooking/baking and, for me, both the art forms evoke the same creative feeling.
Never Graduate
Finally, my batch ended in the first week of August, 2022. Doing a batch at RC was an unique and memorable experience for me. The environment was highly diverse and all Recursers were kind and supportive of each other's goals. I'm grateful to the RC faculty for creating and maintaining this safe and happy place.

As a proud alumni of RC, I'll carry the spirit of RC everywhere I go and look forward to the day I can hangout at RC in-person.  
* Rena's Memory Model
:PROPERTIES:
:RSS_PERMALINK: rena's_memory_model.html
:PUBDATE:  2021-11-25
:ID:       32856908-996c-45d3-81a9-db31dac2efb7
:END:
#+setupfile: ../basic-setup.org
[[https://github.com/veera-sivarajan/rena][Rena]] is my implementation of lox tree-walk interpreter in Rust. I'm rewriting it in Rust to familiarize myself with the borrow checker and the fancy toolchain it offers. In this post, I'll explain how I implemented a simple environment for the interpreter. 
Structure
The environment is structured as a vector of hashmaps and is initialized with an empty hashmap to represent the global scope. Every new block in the source code inserts an empty hashmap at the head of the vector and exiting out of a block removes a hashmap from the head. The environment combines these three properties to support nesting and shadowing of values in the memory.  
#+begin_src rust
  pub struct Environment {
      frame_list: Vec<HashMap<String, Value>>,
  }

  pub fn new() -> Environment {
      let mut frames = Vec::new();
      frames.push(HashMap::new());

      Environment {
          frame_list: frames,
      }
  }

  pub fn new_scope(&mut self) {
      self.frame_list.insert(0, HashMap::new());
  }

  pub fn exit_scope(&mut self) {
      self.frame_list.remove(0);
  }
#+end_src
Declaring New Values
When the interpreter encounters a new block in the parsed source code, it first calls ~new_scope()~ to initialize a new hashmap for the block. So when new values are declared inside the block, they can be safely inserted in the first hashmap of the vector which represents the innermost scope. 
#+begin_src rust
  pub fn define(&mut self, name: String, value: Value) -> Result<(), LoxError> {
      if let Some(frame) = self.frame_list.get_mut(0) {
          frame.insert(name, value);
          Ok(())
      } else {
          error!("Frame not available.")
      }
  }
#+end_src
Fetching and Assigning Values
~fetch()~ is used to fetch a value from the environment and ~assign()~ is used to assign a new value to a predefined key. Both the functions are required to climb up from the starting scope to the global scope until a hashmap containing the given key is found. This is accomplished by iterating through each hashmap in the vector and performing the fetch or assign operation only if a hashmap contains the key. If the key is not found in any hashmap, the value being searched for has not been declared.

For example, to fetch a variable ~a~ declared in the global scope and used inside a block statement, the interpreter will first check the first hashmap in the vector (innermost scope) for the variable name and will not find it. So it will proceed to check the second hashmap (global scope) and since this hashmap contains the variable's key the function will return the value associated with the key.  
#+begin_src rust
  pub fn fetch(&self, name: String) -> Option<&Value> {
      for frame in &self.frame_list {
          if frame.contains_key(&name) {
              return frame.get(&name);
          }
      }
      None
  }

  pub fn assign(&mut self, name: String, value: Value) -> Result<Value, LoxError> {
      for frame in &mut self.frame_list {
          if frame.contains_key(&name) {
              frame.insert(name, value.clone());
              return Ok(value);
          }
      }
      error!("Undefined variable.")
  }
#+end_src
The End
Here's a pictorial representation of the environment for the source code below. The first element represents the innermost scope and the following elements represent the enclosing outer scopes. 
[[file:/home/veera/Projects/Blog/local/imgs/scopes3.png]]
#+begin_src
  var a = 1;
  var b = 2;
  {
      var c = 100;
      print a; // 1
      {
          var d = 4;
          {
              var e = 5;
              c = 3;
              print c; // 3
          }
      }
  }
#+end_src
Compared to the implementation provided in the book, my implementation sits well with the borrow checker and may be more performant and readable as it uses iteration instead of recursion to climb up the nested scopes. 

Now, back to hacking :^)

Edit
A comment by [[https://www.reddit.com/r/rust/comments/r2tblf/comment/hm84mvq/?utm_source=share&utm_medium=web2x&context=3][u/Training-Ad-9425]] mentioned that my implementation is inefficient and after referring to the [[https://doc.rust-lang.org/std/collections/index.html#sequences][documentation]], I realized that inserting and removing an element from the head of a ~Vec~ is a $O(n)$ operation.

To improve on that, I had two options:
Push and pop new scopes at the tail of the vector and reverse the sequence when traversing from inner to outer scope. This would allow new scopes to be added to the environment in $O(1)$ and traversing will remain to be $O(n)$. 
Replace the environment structure with ~VecDeque~ as it supports $O(1)$ insertion and removal at both ends of the sequence.

I implemented the first option as it would give me a chance to play with iterator's methods. You can find the update source code in the remote repository linked above.
* Game of Light
:PROPERTIES:
:RSS_PERMALINK: game_of_light.html
:PUBDATE:  2021-11-01
:ID:       9a8a7a95-af46-4d61-8342-195cd5d31f9b
:END:
#+setupfile: ../basic-setup.org
For our final project, Hayden and I are building [[https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life][Conway's Game of Life]] simulator on a LED matrix. We will also add a controller which the audience can use to pause and play the simulation. With this project, we aim to understand how interesting patterns emerge from a small set of rules. The coordinates for the simulation will be computed on an Arduino board which will be connected to both the simulator and the controller. Along the way, we might also add features that tickles our fancy. Check this page by December, 2021 to view the final product.   
Update
After soldering countless wire tips to the LED matrix and Hayden's all-out effort on porting the [[https://github.com/veera-sivarajan/game-of-light/blob/master/gameOfLight.ino][software]] to run on our hardware, we were able to finish the project on time and demo it during the final presentation. To see how our final project looked like, check out this [[https://youtu.be/IcmwVxqATL4][video]] where we simulate a bunch of still life patterns in the game. 
Fin
CICS 256 has been a great learning experience as we got to work on a project of our choice instead of writing exams. I learned a great deal about electronics and how software and hardware interact with each other. I'm sure the skills I gained from this class will help me throughout my life. CICS 256 rocks! 
* Adding Features To Lox
:PROPERTIES:
:RSS_PERMALINK: adding_features_to_lox.html
:PUBDATE:  2021-08-19
:ID:       07ef7e1b-b073-4420-a6ed-0be2cd31d24d
:END:
#+setupfile: ../basic-setup.org
Finally, I got time to sit down and implement a tree-walk interpreter following Bob Nystrom's [[https://craftinginterpreters.com/][Crafting Interpreters]]. In this post, I'm going to explain some of the features I have added to the language. You can find my complete implementation [[https://github.com/veera-sivarajan/lang0][here]]. I have also made a [[https://youtu.be/0P6VPOIKATg][video]] demoing the features listed below.

*Note:* This post assumes that the reader has read Part II of Crafting Interpreters and is familiar with the implementation details. 
Anonymous Functions
By definition, anonymous functions or lambdas are functions without an associated name. They are used when the user wants to invoke a piece of code but don't want to reuse it. This feature also solves challenge #2 in chapter 10 /Functions/. In my implementation, a lambda can be written by using the ~lambda~ keyword followed by the parameters inside parentheses and the body of the function in braces.
#+begin_src lox
  // lambdas in lox
  var square = lambda(num) { return num * num; };
  print square(5); // 25
#+end_src
A lambda is a primary expression that gets evaluated to a value of type ~LambdaFunction~. Parsing it is exactly like parsing a function declaration but here the parser does not consume a name for the function. Finally, it wraps the parameters and the body of lambda in a AST node. 
#+begin_src cpp
  std::shared_ptr<Expr> Parser::lambda() {
      consume(TokenType::LEFT_PAREN, "Expect '(' after 'lambda'.");
      std::vector<Token> parameters;
      if (!check(TokenType::RIGHT_PAREN)) {
          do {
              if (parameters.size() >= 10) {
                  error(peek(), "Can't have more than 10 parameters.");
              }
              parameters.push_back(
                  consume(TokenType::IDENTIFIER, "Expect parameter name."));
          } while (match(TokenType::COMMA));
      }
      consume(TokenType::RIGHT_PAREN, "Expect ')' after parameters.");
      consume(TokenType::LEFT_BRACE, "Expect '{' before lambda body.");
      std::vector<std::shared_ptr<Statement::Stmt>> body = block();
      return std::make_shared<Lambda>(std::move(parameters), std::move(body));
  }
#+end_src
Since these expressions don't have a name and cannot be reused like a variable or a named function, the interpreter need not store the value of the function in its environment. Instead it would create a runtime's representation of an anonymous function using the parsed value and the environment of the lambda (to allow closures to work) and return the value.
#+begin_src cpp
  std::any Interpreter::visitLambdaExpr(std::shared_ptr<Lambda> expr) {
      return std::make_shared<LambdaFunction>(expr, curr_env);
  }
#+end_src
Lists
To add a list data type into the language, the task can be divided into three small tasks:
** Scanning
:PROPERTIES:
:ID:       654883aa-5bd1-4ed5-814f-a5f475128067
:END:
Introduce two new tokens to make the scanner handle list expressions and subscripts.
#+begin_src cpp
  case '[': addToken(TokenType::LEFT_BRACKET); break;
  case ']': addToken(TokenType::RIGHT_BRACKET); break;
#+end_src
** Creating List Literals
:PROPERTIES:
:ID:       c1fa3703-4489-4be8-9706-3c0832f36dc6
:END:
Like numbers, string, ~true~ and ~false~, a list is a primary expression which can contain any number of comma seperated expressions. Since Lox is a dynamically typed language, these expressions can be of any type. 

To learn about the grammar for lists, I found Caleb's blog [[https://calebschoepp.com/blog/2020/adding-a-list-data-type-to-lox/][post]] to be helpful. It explained all the details related to implementing lists in a clear and concise way.
*** Parsing Lists
:PROPERTIES:
:ID:       6b9a884d-5254-4eb7-b997-f22e6e0f1830
:END:
 Parsing a list literal is exactly like parsing the arguments of a function call but here square brackets replaces the parentheses. Internally, the parser stores a list as a vector containing expressions. The function first checks if it is a empty list. If its not, it parses each expression in the scanned list and appends it to the internal representation. In the end, it consumes the ~]~ and wraps the vector in a AST node.
#+begin_src cpp
  std::shared_ptr<Expr> Parser::list() {
      std::vector<std::shared_ptr<Expr>> values = {};
      if (match(TokenType::RIGHT_BRACKET)) {
          return std::make_shared<List>(values);
      } else {
          do {
              if (values.size() >= 100) {
                  error(peek(), "Can't have more than 100 elements in a list.");
              }
              std::shared_ptr<Expr> value = logicalOr();
              values.push_back(value);
          } while (match(TokenType::COMMA));
      }
      consume(TokenType::RIGHT_BRACKET, "Expect ']' at end of list.");
      return std::make_shared<List>(values);
  }
#+end_src
*** Interpreting Lists
:PROPERTIES:
:ID:       0cc8f545-b19d-45b9-9646-5b26293703af
:END:
To interpret a list literal, the interpreter iterates through each element in the parsed expression, evaluates it and appends the value to runtime's representation of a list. Finally, like interpreting any other kind of expression, the runtime's value is returned. 
#+begin_src cpp
  std::any Interpreter::visitListExpr(std::shared_ptr<List> expr) {
      auto list = std::make_shared<ListType>();
      for (std::shared_ptr<Expr> &value : expr->values) {
          list->append(evaluate(value));
      }
      return list;
  }
#+end_src
** Handling Subscript Expressions
:PROPERTIES:
:ID:       fcd53ada-88b9-4859-b2b1-3b5815fb8b24
:END:
Subscript expressions are used to get and set an element at a particular index in the list. In other words, they can be used both as a l-value and a r-value and to diffrentiate between the two and also to avoid repetition of code I have used a simple trick[fn::I'm not sure if this would work when classes and methods are implemented.].

From the interpreter's point of view, the only difference between the two is the presence of a value expression. If the AST node has a value, it should assign it at index. Otherwise, it should return the value at index. So the trick is to make the parser pass a =nullptr= in the place of value expression when parsing a r-value and pass a value only when parsing a l-value. This allows the interpreter to easily differentiate between the two kinds of subscript expressions.
*** Parsing Subscripts
:PROPERTIES:
:ID:       e33304ee-2c46-40d1-bb26-2db56ef327b2
:END:
Parsing the r-value is, once again, similar to parsing a function call. But instead of parsing any number of arguments, the parser will only parse a single value between the square brackets. ~subscript()~ calls ~finishSubscript()~ each time it sees a ~[~ to support indexing list of lists. Lastly, to wrap everything in a node, the parser stores a =nullptr= in the place of value expression to let the interpreter know that it is a r-value. 
#+begin_src cpp
  std::shared_ptr<Expr> Parser::finishSubscript(std::shared_ptr<Expr> name) {
      std::shared_ptr<Expr> index = logicalOr();
      Token paren = consume(TokenType::RIGHT_BRACKET,
                            "Expect ']' after arguments.");
      return std::make_shared<Subscript>(name, index, nullptr, paren);
  }

  std::shared_ptr<Expr> Parser::subscript() {
      std::shared_ptr<Expr> expr = primary();
      while (true) {
          if (match(TokenType::LEFT_BRACKET)) {
              expr = finishSubscript(expr);
          } else {
              break;
          }
      }
      return expr;
  }
#+end_src
As you may have noticed in the formal grammar, for parsing a l-value expression, the parser extends the ~assignment~ rule. If the parsed expression is of type ~Subscript~, it wraps the list's name, index and value expression in a AST node. The list's name is stored as an expression without converting it into l-value because the methods used for manipulating the list act on a evaluated value of the list. 
#+begin_src cpp
  std::shared_ptr<Expr> Parser::assignment() {
      std::shared_ptr<Expr> expr = logicalOr();
      if (match(TokenType::EQUAL)) {
          Token equals = previous();
          std::shared_ptr<Expr> value = assignment();
          // parse variable assignment
          } else if (Subscript *s = dynamic_cast<Subscript *>(expr.get())) {
              return std::make_shared<Subscript>(s->name, s->index, value,
                                                 s->paren);
          }
          error(std::move(equals), "Invalid assignment target.");
      }
      return expr;
  }
#+end_src
*** Interpreting Subscripts
:PROPERTIES:
:ID:       f04e1fb0-2ff3-4796-9fc8-9d31404901e2
:END:
To interpret a subscript expression, the interpreter first evaluates the list's name and index and checks if they are of the correct type. If they pass the type checks, the interpreter casts the list's name to the runtime's representation of a list and the index to ~int~ from ~double~[fn::This is actually a bug because index number should not be of type ~double~]. Checking if the index is out of range is done at the last moment as they should be handled differently for l-value and r-value. 

Now the interpreter knows that the list and the index are of valid types and is ready interpret it. If the AST node has a value expression, the interpreter evaluates it and assigns it at the index. List's ~setAtIndex()~ method sets a value at a index under two condtions:
When index is equivalent to length of list: To append a value to the list.
When index is less than length of list and greater than zero: To assign a value at a index. 
If neither of those conditions evaluate to a truthy value, the method returns ~false~ and the interpreter throws a runtime error. 

If the node does *not* have a value it's a r-value and the interpreter is supposed to return the value at index. It returns the value using the list's method if index is within the range of the list. Otherwise, if the index is out of range, it returns a =nullptr=.
#+begin_src cpp
  std::any Interpreter::visitSubscriptExpr(std::shared_ptr<Subscript> expr) {
      std::any name = evaluate(expr->name);
      std::any index = evaluate(expr->index);
      if (name.type() == typeid(std::shared_ptr<ListType>)) {
          if (index.type() == typeid(double)) {
              std::shared_ptr<ListType> list;
              int castedIndex;
              list = std::any_cast<std::shared_ptr<ListType>>(name);
              castedIndex = std::any_cast<double>(index);
              if (expr->value != nullptr) {
                  std::any value = evaluate(expr->value);
                  if (list->setAtIndex(castedIndex, value)) {
                      return value; 
                  } else {
                      throw RuntimeError{expr->paren, "Index out of range."};
                  }
              } else {
                  if (castedIndex >= list->length() || castedIndex < 0) {
                      return nullptr;
                  }
                  return list->getEleAt(castedIndex);
              }
          } else {
              throw RuntimeError{expr->paren, "Index should be of type int."};
          }
      } else {
          throw RuntimeError{expr->paren, "Only lists can be subscripted."};
      }
      return {};
  }
#+end_src
I could have made the interpreter throw errors when indexing out of range but chose to return a =nullptr= because it helps in terminating a loop while iterating over a list. It might seem unsafe but the interpreter throws an error when a variable initialized with =nullptr= is used in an expression. So if the user tries to access a value out of range and uses it in some other expression, the program is guaranteed to fail. 
#+begin_src lox
  // lox script to print all elements in a list
  var list = [1, 2, 3, 4, 5];
  for (var i = 0; list[i] != nil; i = i + 1) {
      print list[i]; // 1 2 3 4 5
  }
#+end_src
Unused Variable Warnings
I also made the resolver throw warnings when there are unused variables in the local scope. This also solves challenge #3 in chapter 11 /Resolving and Binding/.

To implement this feature, a vector of ~std::map~ is used like a stack to track the nested (possibly) local scopes in scope. This is similar to the =scopes= stack used in the resolver but instead of storing a string and a boolean, here I store the variable as a token and the number of times it has been resolved. When the resolver enters a local scope it pushes an empty map into the vector and when it exits a scope it pops a map. Declaring a variable in the local scope inserts a element with variable token as key and =0= as value in the top most map. This value gets incremented by one whenever the corresponding variable is resolved locally. Finally, before exiting a local scope calling =checkUnusedVariables()= would iterate over all the pairs in the top most map and throw a warning when the value is equivalent to zero.
#+begin_src cpp
  void Resolver::checkUnusedVariables() {
      std::map<Token, int> &currentScope = identifiers.back();
      for (auto const& [key, val] : currentScope) {
          if (val == 0) {
              Error::warn(key, "Unused local variable.");
          }
      }
  }
#+end_src
Epilogue
I had so much fun working on this project and learnt a lot about programming languages and interpreters. Before reading this book, I did not know what actually happens when I run or compile my programs but now I have a better understanding of the underlying ideas and visualize a interpreter as follows:

Scanner:
Converts user input into list of tokens.
Parser:
Tokens into expression or statement AST node based on formal grammar.
Interpreter:
Evaluates a expression node to a value.
Executes a statement node to produce side effect.
Environment:
Stores the state of the program.

Next, I'm looking forward to start working on the bytecode interpreter.

